{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 773,
     "status": "ok",
     "timestamp": 1612283969035,
     "user": {
      "displayName": "Reinel Tabares Soto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjKTA6QArEnPRuOWf7OPNtvknz1x59b02IBfrnLEamjl6wujToaOXXTLypLDaeE7DxeMdruWvVVA65QvseNhkbr-rfOYlotIFM8SalLQ9i3nBZc_u4drgOlZeYhcy2aGVEMkGqNlUr7E7CjrzK8zu8F_L0jz4ibAM3Ot9nUdkx4pqtZ0laRTObrRNFtmpJB49dArGRqi1hKywSX3-FoHaapJNW7UcpHMVhCvKYFDveOS2te2wr_t3oEQGn9aO96BFLkLj4XLprnzZ2G8bioWEDf5yMbZtOjRWcQ6iwvBtR2_IkhXRpARqwhb4QWw2oLsCaKmdOOIWGk-tqBv_4PnY_iKboo5yBNgnhUU798EMk7rPhsYFfFRZIXkLbtlVtAypYtLYjSkyLb9JuKDeHw90SZSJpxD4-FpCYPlW5LrlYEhbE6e_1l026JSR0zXlhYqsuBGEeWCTrfGgnA7ytK6Kn1uGENyrfPUTXYYEPy5PKeKq4kMwkZwGxO1ofRyvQ2FtjYVm6EG15xQkMd5fv7FYBgPRY9diBq7_JuDkDNHKf3F7XxxtFISPSnA0mJpd6kAi-wjWz5cgV0Iw6R_jrRRLvsoxRS2jKBrBza1YrsgPXM5pdvveWJBIU1TBdVPcseZnr04cEoqS7bPQGI8ZA0IgPBUFb3NyTrNbUUuD8IIIJZV7abgTKIiGU9EVQ1un6Wgc3EhVyve78ge9Au8x-DFOcMQQNHoQLLQR2mCmQrzhOsTXllhI-FoSrn_K_0qwJJIKlDeg=s64",
      "userId": "11849440450678359784"
     },
     "user_tz": 300
    },
    "id": "Dr68WrvRmwbG",
    "outputId": "9dac2def-b528-4807-cdd4-95eea222fda1"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from pandas import set_option\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import seaborn as sns; sns.set() \n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "import re\n",
    "#from sklearn.preprocessing import Imputer\n",
    "from numpy import random\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import cycle\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from scipy.stats import percentileofscore\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import os\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "#from yellowbrick.style.palettes import LINE_COLOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.javaer101.com/article/2986770.html\n",
    "#https://www.kite.com/python/answers/how-to-compute-the-confidence-interval-of-a-sample-statistic-in-python\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.stats import sem\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def ROC_CI_y_true_y_pred(y_true,y_pred):\n",
    "    roc_auc = auc(y_true, y_pred)\n",
    "    #print(\"Original ROC area: {:0.3f}\".format(roc_auc))\n",
    "\n",
    "    n_bootstraps = 1000\n",
    "    rng_seed = 42  # control reproducibility\n",
    "    bootstrapped_scores = []\n",
    "\n",
    "    rng = np.random.RandomState(rng_seed)\n",
    "    for i in range(n_bootstraps):\n",
    "        # bootstrap by sampling with replacement on the prediction indices\n",
    "        indices = rng.randint(0, len(y_pred), len(y_pred))\n",
    "        if len(np.unique(y_true[indices])) < 2:\n",
    "            # We need at least one positive and one negative sample for ROC AUC\n",
    "            # to be defined: reject the sample\n",
    "            continue\n",
    "\n",
    "        score = auc(y_true[indices], y_pred[indices])\n",
    "        bootstrapped_scores.append(score)\n",
    "        #print(\"Bootstrap #{} ROC area: {:0.3f}\".format(i + 1, score))\n",
    "    sample = np.array(bootstrapped_scores)\n",
    "    confidence_level = 0.95\n",
    "    degrees_freedom = sample.size - 1\n",
    "    sample_mean = np.mean(sample)\n",
    "    sample_standard_error = scipy.stats.sem(sample)\n",
    "\n",
    "    confidence_interval = scipy.stats.t.interval(confidence_level, degrees_freedom, sample_mean, sample_standard_error)\n",
    "    min_ = confidence_interval[0]\n",
    "    max_ = confidence_interval[1]\n",
    "    return roc_auc, min_, max_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_ci(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    score_fun,\n",
    "    n_bootstraps=2000,\n",
    "    confidence_level=0.95,\n",
    "    seed=None,\n",
    "    reject_one_class_samples=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute confidence interval for given score function based on labels and predictions using bootstrapping.\n",
    "    :param y_true: 1D list or array of labels.\n",
    "    :param y_pred: 1D list or array of predictions corresponding to elements in y_true.\n",
    "    :param score_fun: Score function for which confidence interval is computed. (e.g. sklearn.metrics.accuracy_score)\n",
    "    :param n_bootstraps: The number of bootstraps. (default: 2000)\n",
    "    :param confidence_level: Confidence level for computing confidence interval. (default: 0.95)\n",
    "    :param seed: Random seed for reproducibility. (default: None)\n",
    "    :param reject_one_class_samples: Whether to reject bootstrapped samples with only one label. For scores like AUC we\n",
    "    need at least one positive and one negative sample. (default: True)\n",
    "    :return: Score evaluated on labels and predictions, lower confidence interval, upper confidence interval, array of\n",
    "    bootstrapped scores.\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(y_true) == len(y_pred)\n",
    "\n",
    "    score = score_fun(y_true, y_pred)\n",
    "    _, ci_lower, ci_upper, scores = score_stat_ci(\n",
    "        y_true=y_true,\n",
    "        y_preds=y_pred,\n",
    "        score_fun=score_fun,\n",
    "        n_bootstraps=n_bootstraps,\n",
    "        confidence_level=confidence_level,\n",
    "        seed=seed,\n",
    "        reject_one_class_samples=reject_one_class_samples,\n",
    "    )\n",
    "\n",
    "    return score, ci_lower, ci_upper, scores\n",
    "\n",
    "\n",
    "def score_stat_ci(\n",
    "    y_true,\n",
    "    y_preds,\n",
    "    score_fun,\n",
    "    stat_fun=np.mean,\n",
    "    n_bootstraps=2000,\n",
    "    confidence_level=0.95,\n",
    "    seed=None,\n",
    "    reject_one_class_samples=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute confidence interval for given statistic of a score function based on labels and predictions using\n",
    "    bootstrapping.\n",
    "    :param y_true: 1D list or array of labels.\n",
    "    :param y_preds: A list of lists or 2D array of predictions corresponding to elements in y_true.\n",
    "    :param score_fun: Score function for which confidence interval is computed. (e.g. sklearn.metrics.accuracy_score)\n",
    "    :param stat_fun: Statistic for which confidence interval is computed. (e.g. np.mean)\n",
    "    :param n_bootstraps: The number of bootstraps. (default: 2000)\n",
    "    :param confidence_level: Confidence level for computing confidence interval. (default: 0.95)\n",
    "    :param seed: Random seed for reproducibility. (default: None)\n",
    "    :param reject_one_class_samples: Whether to reject bootstrapped samples with only one label. For scores like AUC we\n",
    "    need at least one positive and one negative sample. (default: True)\n",
    "    :return: Mean score statistic evaluated on labels and predictions, lower confidence interval, upper confidence\n",
    "    interval, array of bootstrapped scores.\n",
    "    \"\"\"\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_preds = np.atleast_2d(y_preds)\n",
    "    assert all(len(y_true) == len(y) for y in y_preds)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    scores = []\n",
    "    for i in range(n_bootstraps):\n",
    "        readers = np.random.randint(0, len(y_preds), len(y_preds))\n",
    "        indices = np.random.randint(0, len(y_true), len(y_true))\n",
    "        if reject_one_class_samples and len(np.unique(y_true[indices])) < 2:\n",
    "            continue\n",
    "        reader_scores = []\n",
    "        for r in readers:\n",
    "            reader_scores.append(score_fun(y_true[indices], y_preds[r][indices]))\n",
    "        scores.append(stat_fun(reader_scores))\n",
    "\n",
    "    mean_score = np.mean(scores)\n",
    "    sorted_scores = np.array(sorted(scores))\n",
    "    alpha = (1.0 - confidence_level) / 2.0\n",
    "    ci_lower = sorted_scores[int(round(alpha * len(sorted_scores)))]\n",
    "    ci_upper = sorted_scores[int(round((1.0 - alpha) * len(sorted_scores)))]\n",
    "    return mean_score, ci_lower, ci_upper, scores\n",
    "\n",
    "\n",
    "def pvalue(\n",
    "    y_true,\n",
    "    y_pred1,\n",
    "    y_pred2,\n",
    "    score_fun,\n",
    "    n_bootstraps=2000,\n",
    "    two_tailed=True,\n",
    "    seed=None,\n",
    "    reject_one_class_samples=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute p-value for hypothesis that score function for model I predictions is higher than for model II predictions\n",
    "    using bootstrapping.\n",
    "    :param y_true: 1D list or array of labels.\n",
    "    :param y_pred1: 1D list or array of predictions for model I corresponding to elements in y_true.\n",
    "    :param y_pred2: 1D list or array of predictions for model II corresponding to elements in y_true.\n",
    "    :param score_fun: Score function for which confidence interval is computed. (e.g. sklearn.metrics.accuracy_score)\n",
    "    :param n_bootstraps: The number of bootstraps. (default: 2000)\n",
    "    :param two_tailed: Whether to use two-tailed test. (default: True)\n",
    "    :param seed: Random seed for reproducibility. (default: None)\n",
    "    :param reject_one_class_samples: Whether to reject bootstrapped samples with only one label. For scores like AUC we\n",
    "    need at least one positive and one negative sample. (default: True)\n",
    "    :return: Computed p-value, array of bootstrapped differences of scores.\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(y_true) == len(y_pred1)\n",
    "    assert len(y_true) == len(y_pred2)\n",
    "\n",
    "    return pvalue_stat(\n",
    "        y_true=y_true,\n",
    "        y_preds1=y_pred1,\n",
    "        y_preds2=y_pred2,\n",
    "        score_fun=score_fun,\n",
    "        n_bootstraps=n_bootstraps,\n",
    "        two_tailed=two_tailed,\n",
    "        seed=seed,\n",
    "        reject_one_class_samples=reject_one_class_samples,\n",
    "    )\n",
    "\n",
    "\n",
    "def pvalue_stat(\n",
    "    y_true,\n",
    "    y_preds1,\n",
    "    y_preds2,\n",
    "    score_fun,\n",
    "    stat_fun=np.mean,\n",
    "    n_bootstraps=2000,\n",
    "    two_tailed=True,\n",
    "    seed=None,\n",
    "    reject_one_class_samples=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute p-value for hypothesis that given statistic of score function for model I predictions is higher than for\n",
    "    model II predictions using bootstrapping.\n",
    "    :param y_true: 1D list or array of labels.\n",
    "    :param y_preds1: A list of lists or 2D array of predictions for model I corresponding to elements in y_true.\n",
    "    :param y_preds2: A list of lists or 2D array of predictions for model II corresponding to elements in y_true.\n",
    "    :param score_fun: Score function for which confidence interval is computed. (e.g. sklearn.metrics.accuracy_score)\n",
    "    :param stat_fun: Statistic for which p-value is computed. (e.g. np.mean)\n",
    "    :param n_bootstraps: The number of bootstraps. (default: 2000)\n",
    "    :param two_tailed: Whether to use two-tailed test. (default: True)\n",
    "    :param seed: Random seed for reproducibility. (default: None)\n",
    "    :param reject_one_class_samples: Whether to reject bootstrapped samples with only one label. For scores like AUC we\n",
    "    need at least one positive and one negative sample. (default: True)\n",
    "    :return: Computed p-value, array of bootstrapped differences of scores.\n",
    "    \"\"\"\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_preds1 = np.atleast_2d(y_preds1)\n",
    "    y_preds2 = np.atleast_2d(y_preds2)\n",
    "    assert all(len(y_true) == len(y) for y in y_preds1)\n",
    "    assert all(len(y_true) == len(y) for y in y_preds2)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    z = []\n",
    "    for i in range(n_bootstraps):\n",
    "        readers1 = np.random.randint(0, len(y_preds1), len(y_preds1))\n",
    "        readers2 = np.random.randint(0, len(y_preds2), len(y_preds2))\n",
    "        indices = np.random.randint(0, len(y_true), len(y_true))\n",
    "        if reject_one_class_samples and len(np.unique(y_true[indices])) < 2:\n",
    "            continue\n",
    "        reader_scores = []\n",
    "        for r in readers1:\n",
    "            reader_scores.append(score_fun(y_true[indices], y_preds1[r][indices]))\n",
    "        score1 = stat_fun(reader_scores)\n",
    "        reader_scores = []\n",
    "        for r in readers2:\n",
    "            reader_scores.append(score_fun(y_true[indices], y_preds2[r][indices]))\n",
    "        score2 = stat_fun(reader_scores)\n",
    "        z.append(score1 - score2)\n",
    "\n",
    "    p = percentileofscore(z, 0.0, kind=\"weak\") / 100.0\n",
    "    if two_tailed:\n",
    "        p *= 2.0\n",
    "    return p, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n",
    "#https://github.com/BioAITeam/Steganalysis/blob/main/MV.py\n",
    "#https://github.com/BioAITeam/Steganalysis/blob/main/GBRAS-Net.ipynb\n",
    "#https://github.com/mateuszbuda/ml-stat-util/blob/master/stat_util.py\n",
    "#https://pubs.rsna.org/doi/pdf/10.1148/radiol.2020202944\n",
    "\n",
    "def ROC_curves(y_test,y_score,name, path_img_base = './images'):\n",
    "    if not os.path.exists(path_img_base):\n",
    "        os.makedirs(path_img_base)\n",
    "    n_classes = y_test.shape[1]\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        \n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    \n",
    "    #roc_auc, min_, max_ = ROC_CI_y_true_y_pred(tpr[\"micro\"],fpr[\"micro\"])\n",
    "    #print(roc_auc, min_, max_) \n",
    "    \n",
    "    # Compute macro-average ROC curve and ROC area\n",
    "    # First aggregate all false positive rates\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "    \n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    classes_names=[\"DB\",\"SS\",\"SW\",\"A\",\"I\",\"B\"]\n",
    "    for i in range(n_classes):\n",
    "        score, ci_lower, ci_upper, scores = score_ci(y_test[:, i], y_score[:, i], \n",
    "                                                       score_fun=roc_auc_score,\n",
    "                                                       seed=42)\n",
    "        \n",
    "        text=\"ROC curve with AUC of {:.3f} (95% CI: {:.3f}, {:.3f}). \"+\"Class \"+classes_names[i]\n",
    "\n",
    "        plt.plot(fpr[i], tpr[i], \n",
    "                 label=text.format(score, ci_lower, ci_upper))\n",
    "    \n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "             label='micro-average ROC curve (AUC = {0:0.3f})'.format(roc_auc[\"micro\"]),\n",
    "             color='aqua', linestyle=':', linewidth=4)\n",
    "\n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "             label='macro-average ROC curve (AUC = {0:0.3f})'.format(roc_auc[\"macro\"]),\n",
    "             color='darkorange', linestyle=':', linewidth=4)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\":\", c=LINE_COLOR) \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.tick_params(axis='x', which='major', labelsize=16) \n",
    "    plt.tick_params(axis='y', which='major', labelsize=16)\n",
    "    plt.xlabel('False Positive Rate', fontsize=14)\n",
    "    plt.ylabel('True Positive Rate', fontsize=14)\n",
    "    plt.title(\"ROC Curves for {}\".format(name), fontsize=16)\n",
    "    plt.legend(loc=\"lower right\", frameon=True, fontsize=15)\n",
    "    plt.savefig(path_img_base+'/ROC_CI_graphs_per_class_'+name+'.svg', format='svg') \n",
    "    plt.savefig(path_img_base+'/ROC_CI_graphs_per_class_'+name+'.pdf', format='pdf') \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shoham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = ExtraTreesClassifier(n_estimators=112, max_depth=31, min_samples_split=5, random_state=28000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA = '../Databases/ShohamDB.csv' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading Data\n",
    "\n",
    "# Velocity, Viscosity, Density, Surface Tension, Angle and Diameter\n",
    "dataset = pd.DataFrame(pd.read_csv(PATH_DATA), columns=['Vsl', 'Vsg', 'VisL', 'VisG', 'DenL', 'DenG', 'ST', 'Ang', 'ID', 'Flow Pattern']) \n",
    "\n",
    "# Summarize the Dataset \n",
    "print(\"shape of initial data =\",dataset.shape) \n",
    "# Class Distribution \n",
    "print(dataset.groupby('Flow Pattern').size()) \n",
    "# Leaving only the best training variables\n",
    "dataset = dataset.drop(['VisG', 'VisL','DenG', 'ST', 'DenL'], axis=1) #Delete this variables\n",
    "print(\"shape of selected data =\",dataset.shape) \n",
    "\n",
    "print(dataset.head()) \n",
    "\n",
    "# Split-out validation dataset \n",
    "array = dataset.values \n",
    "X = array[:,0:4] #Data or features \n",
    "y = array[:,4]   #Label or classes \n",
    "\n",
    "validation_size = 0.20\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=validation_size, random_state=510) \n",
    "\n",
    "print(\"\\ntrain data shape =\",X_train.shape) \n",
    "print(\"train labels shape =\",y_train.shape) \n",
    "print(\"test data shape =\",X_test.shape) \n",
    "print(\"test labels shape =\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "model = model_\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "acc_score=accuracy_score(y_test,y_pred) \n",
    "print('accuracy_score: {0:.4f}'.format(acc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# define example\n",
    "data = y\n",
    "values = array(data)\n",
    "print(values)\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "print(integer_encoded)\n",
    "print(integer_encoded.shape)\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)\n",
    "print(onehot_encoded.shape)\n",
    "# invert first example\n",
    "inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n",
    "print(inverted)\n",
    "\n",
    "\n",
    "validation_size = 0.20\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, onehot_encoded, test_size=validation_size, random_state=510) \n",
    "print(\"\\ntrain data shape =\",X_train.shape) \n",
    "print(\"train labels shape =\",y_train.shape) \n",
    "print(\"test data shape =\",X_test.shape) \n",
    "print(\"test labels shape =\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "model = model_\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "acc_score=accuracy_score(y_test,y_pred) \n",
    "print('accuracy_score: {0:.4f}'.format(acc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = model_\n",
    "clf.fit(X_train,y_train)\n",
    "y_score = clf.predict_proba(X_test)\n",
    "print(np.array(y_score).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(y_score).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1609,
     "status": "ok",
     "timestamp": 1612287787803,
     "user": {
      "displayName": "Reinel Tabares Soto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjKTA6QArEnPRuOWf7OPNtvknz1x59b02IBfrnLEamjl6wujToaOXXTLypLDaeE7DxeMdruWvVVA65QvseNhkbr-rfOYlotIFM8SalLQ9i3nBZc_u4drgOlZeYhcy2aGVEMkGqNlUr7E7CjrzK8zu8F_L0jz4ibAM3Ot9nUdkx4pqtZ0laRTObrRNFtmpJB49dArGRqi1hKywSX3-FoHaapJNW7UcpHMVhCvKYFDveOS2te2wr_t3oEQGn9aO96BFLkLj4XLprnzZ2G8bioWEDf5yMbZtOjRWcQ6iwvBtR2_IkhXRpARqwhb4QWw2oLsCaKmdOOIWGk-tqBv_4PnY_iKboo5yBNgnhUU798EMk7rPhsYFfFRZIXkLbtlVtAypYtLYjSkyLb9JuKDeHw90SZSJpxD4-FpCYPlW5LrlYEhbE6e_1l026JSR0zXlhYqsuBGEeWCTrfGgnA7ytK6Kn1uGENyrfPUTXYYEPy5PKeKq4kMwkZwGxO1ofRyvQ2FtjYVm6EG15xQkMd5fv7FYBgPRY9diBq7_JuDkDNHKf3F7XxxtFISPSnA0mJpd6kAi-wjWz5cgV0Iw6R_jrRRLvsoxRS2jKBrBza1YrsgPXM5pdvveWJBIU1TBdVPcseZnr04cEoqS7bPQGI8ZA0IgPBUFb3NyTrNbUUuD8IIIJZV7abgTKIiGU9EVQ1un6Wgc3EhVyve78ge9Au8x-DFOcMQQNHoQLLQR2mCmQrzhOsTXllhI-FoSrn_K_0qwJJIKlDeg=s64",
      "userId": "11849440450678359784"
     },
     "user_tz": 300
    },
    "id": "wcFSbTttszm8"
   },
   "outputs": [],
   "source": [
    "t0=y_score[0][:,1]\n",
    "t1=y_score[1][:,1]\n",
    "t2=y_score[2][:,1]\n",
    "t3=y_score[3][:,1]\n",
    "t4=y_score[4][:,1]\n",
    "t5=y_score[5][:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 715,
     "status": "ok",
     "timestamp": 1612287789099,
     "user": {
      "displayName": "Reinel Tabares Soto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjKTA6QArEnPRuOWf7OPNtvknz1x59b02IBfrnLEamjl6wujToaOXXTLypLDaeE7DxeMdruWvVVA65QvseNhkbr-rfOYlotIFM8SalLQ9i3nBZc_u4drgOlZeYhcy2aGVEMkGqNlUr7E7CjrzK8zu8F_L0jz4ibAM3Ot9nUdkx4pqtZ0laRTObrRNFtmpJB49dArGRqi1hKywSX3-FoHaapJNW7UcpHMVhCvKYFDveOS2te2wr_t3oEQGn9aO96BFLkLj4XLprnzZ2G8bioWEDf5yMbZtOjRWcQ6iwvBtR2_IkhXRpARqwhb4QWw2oLsCaKmdOOIWGk-tqBv_4PnY_iKboo5yBNgnhUU798EMk7rPhsYFfFRZIXkLbtlVtAypYtLYjSkyLb9JuKDeHw90SZSJpxD4-FpCYPlW5LrlYEhbE6e_1l026JSR0zXlhYqsuBGEeWCTrfGgnA7ytK6Kn1uGENyrfPUTXYYEPy5PKeKq4kMwkZwGxO1ofRyvQ2FtjYVm6EG15xQkMd5fv7FYBgPRY9diBq7_JuDkDNHKf3F7XxxtFISPSnA0mJpd6kAi-wjWz5cgV0Iw6R_jrRRLvsoxRS2jKBrBza1YrsgPXM5pdvveWJBIU1TBdVPcseZnr04cEoqS7bPQGI8ZA0IgPBUFb3NyTrNbUUuD8IIIJZV7abgTKIiGU9EVQ1un6Wgc3EhVyve78ge9Au8x-DFOcMQQNHoQLLQR2mCmQrzhOsTXllhI-FoSrn_K_0qwJJIKlDeg=s64",
      "userId": "11849440450678359784"
     },
     "user_tz": 300
    },
    "id": "qppBhdihyUDp"
   },
   "outputs": [],
   "source": [
    "p=[t0,t1,t2,t3,t4,t5]\n",
    "np.array(p).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 502,
     "status": "ok",
     "timestamp": 1612287789316,
     "user": {
      "displayName": "Reinel Tabares Soto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjKTA6QArEnPRuOWf7OPNtvknz1x59b02IBfrnLEamjl6wujToaOXXTLypLDaeE7DxeMdruWvVVA65QvseNhkbr-rfOYlotIFM8SalLQ9i3nBZc_u4drgOlZeYhcy2aGVEMkGqNlUr7E7CjrzK8zu8F_L0jz4ibAM3Ot9nUdkx4pqtZ0laRTObrRNFtmpJB49dArGRqi1hKywSX3-FoHaapJNW7UcpHMVhCvKYFDveOS2te2wr_t3oEQGn9aO96BFLkLj4XLprnzZ2G8bioWEDf5yMbZtOjRWcQ6iwvBtR2_IkhXRpARqwhb4QWw2oLsCaKmdOOIWGk-tqBv_4PnY_iKboo5yBNgnhUU798EMk7rPhsYFfFRZIXkLbtlVtAypYtLYjSkyLb9JuKDeHw90SZSJpxD4-FpCYPlW5LrlYEhbE6e_1l026JSR0zXlhYqsuBGEeWCTrfGgnA7ytK6Kn1uGENyrfPUTXYYEPy5PKeKq4kMwkZwGxO1ofRyvQ2FtjYVm6EG15xQkMd5fv7FYBgPRY9diBq7_JuDkDNHKf3F7XxxtFISPSnA0mJpd6kAi-wjWz5cgV0Iw6R_jrRRLvsoxRS2jKBrBza1YrsgPXM5pdvveWJBIU1TBdVPcseZnr04cEoqS7bPQGI8ZA0IgPBUFb3NyTrNbUUuD8IIIJZV7abgTKIiGU9EVQ1un6Wgc3EhVyve78ge9Au8x-DFOcMQQNHoQLLQR2mCmQrzhOsTXllhI-FoSrn_K_0qwJJIKlDeg=s64",
      "userId": "11849440450678359784"
     },
     "user_tz": 300
    },
    "id": "JVvJ-xxAzh8l"
   },
   "outputs": [],
   "source": [
    "w=np.asmatrix(p).T\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 317,
     "status": "ok",
     "timestamp": 1612287789974,
     "user": {
      "displayName": "Reinel Tabares Soto",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjKTA6QArEnPRuOWf7OPNtvknz1x59b02IBfrnLEamjl6wujToaOXXTLypLDaeE7DxeMdruWvVVA65QvseNhkbr-rfOYlotIFM8SalLQ9i3nBZc_u4drgOlZeYhcy2aGVEMkGqNlUr7E7CjrzK8zu8F_L0jz4ibAM3Ot9nUdkx4pqtZ0laRTObrRNFtmpJB49dArGRqi1hKywSX3-FoHaapJNW7UcpHMVhCvKYFDveOS2te2wr_t3oEQGn9aO96BFLkLj4XLprnzZ2G8bioWEDf5yMbZtOjRWcQ6iwvBtR2_IkhXRpARqwhb4QWw2oLsCaKmdOOIWGk-tqBv_4PnY_iKboo5yBNgnhUU798EMk7rPhsYFfFRZIXkLbtlVtAypYtLYjSkyLb9JuKDeHw90SZSJpxD4-FpCYPlW5LrlYEhbE6e_1l026JSR0zXlhYqsuBGEeWCTrfGgnA7ytK6Kn1uGENyrfPUTXYYEPy5PKeKq4kMwkZwGxO1ofRyvQ2FtjYVm6EG15xQkMd5fv7FYBgPRY9diBq7_JuDkDNHKf3F7XxxtFISPSnA0mJpd6kAi-wjWz5cgV0Iw6R_jrRRLvsoxRS2jKBrBza1YrsgPXM5pdvveWJBIU1TBdVPcseZnr04cEoqS7bPQGI8ZA0IgPBUFb3NyTrNbUUuD8IIIJZV7abgTKIiGU9EVQ1un6Wgc3EhVyve78ge9Au8x-DFOcMQQNHoQLLQR2mCmQrzhOsTXllhI-FoSrn_K_0qwJJIKlDeg=s64",
      "userId": "11849440450678359784"
     },
     "user_tz": 300
    },
    "id": "8vXQy1zQ0IBu"
   },
   "outputs": [],
   "source": [
    "y_score=np.squeeze(np.asarray(w))\n",
    "y_score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test)\n",
    "print(y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test.shape,y_score.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run twice\n",
    "from yellowbrick.style.palettes import LINE_COLOR\n",
    "\n",
    "name = \"Extra Trees model using Shoham data\"\n",
    "path = \"./images/Shoham\"\n",
    "ROC_curves(y_test,y_score,name,path_img_base=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(nrows=2,ncols=2,figsize=(20,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from yellowbrick.style.palettes import LINE_COLOR\n",
    "\n",
    "name = \"Extra Trees model using Shoham data\"\n",
    "path_img_base = \"./images/Shoham\"\n",
    "if not os.path.exists(path_img_base):\n",
    "    os.makedirs(path_img_base)\n",
    "n_classes = y_test.shape[1]\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "#roc_auc, min_, max_ = ROC_CI_y_true_y_pred(tpr[\"micro\"],fpr[\"micro\"])\n",
    "#print(roc_auc, min_, max_) \n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "\n",
    "# Plot ROC curve\n",
    "#plt.figure(figsize=(10, 10))\n",
    "\n",
    "classes_names=[\"DB\",\"SS\",\"SW\",\"A\",\"I\",\"B\"]\n",
    "for i in range(n_classes):\n",
    "    score, ci_lower, ci_upper, scores = score_ci(y_test[:, i], y_score[:, i], \n",
    "                                                   score_fun=roc_auc_score,\n",
    "                                                   seed=42)\n",
    "\n",
    "    text=\"ROC curve with AUC of {:.3f} (95% CI: {:.3f}, {:.3f}). \"+\"Class \"+classes_names[i]\n",
    "\n",
    "    ax[0,0].plot(fpr[i], tpr[i], \n",
    "             label=text.format(score, ci_lower, ci_upper))\n",
    "\n",
    "ax[0,0].plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (AUC = {0:0.3f})'.format(roc_auc[\"micro\"]),\n",
    "         color='aqua', linestyle=':', linewidth=4)\n",
    "\n",
    "ax[0,0].plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (AUC = {0:0.3f})'.format(roc_auc[\"macro\"]),\n",
    "         color='darkorange', linestyle=':', linewidth=4)\n",
    "\n",
    "ax[0,0].tick_params(axis='x', which='major', labelsize=16) \n",
    "ax[0,0].tick_params(axis='y', which='major', labelsize=16)\n",
    "ax[0,0].plot([0, 1], [0, 1], linestyle=\":\", c=LINE_COLOR) \n",
    "ax[0,0].set_xlim([0.0, 1.0])\n",
    "ax[0,0].set_ylim([0.0, 1.0])\n",
    "ax[0,0].set_xlabel('False Positive Rate', fontsize=17)\n",
    "ax[0,0].set_ylabel('True Positive Rate', fontsize=17)\n",
    "ax[0,0].set_title(\"ROC Curves for {}\".format(name), fontsize=18)\n",
    "ax[0,0].legend(loc=\"lower right\", frameon=True, fontsize=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_ = ExtraTreesClassifier(n_estimators=220, max_depth=None, min_samples_split=3, random_state=18) #6\n",
    "#model_ = ExtraTreesClassifier(n_estimators=100, max_depth=None, min_samples_split=3, random_state=17) #5\n",
    "#model_ = ExtraTreesClassifier(n_estimators=100, max_depth=None, min_samples_split=2, random_state=38) #3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = ExtraTreesClassifier(n_estimators=220, max_depth=None, min_samples_split=3, random_state=18) #6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "PATH = '../Databases/12DB_6FP.csv'\n",
    "\n",
    "Data = pd.read_csv(PATH, names=['Vsl', 'Vsg', 'VisL', 'VisG', 'DenL', 'DenG', 'ST', 'Ang', 'ID', 'Flow Pattern'], header=0)\n",
    "print('Data shape:', Data.shape)\n",
    "Data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the class distribution\n",
    "Data['Flow Pattern'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Train, test split\n",
    "features_list = ['Vsl', 'Vsg', 'VisL', 'VisG', 'DenL', 'DenG', 'ST', 'Ang', 'ID']\n",
    "Features = Data[features_list]\n",
    "Labels = Data['Flow Pattern']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Features, Labels, test_size=0.2, stratify=Labels, random_state=42)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "print('Train data shape:', X_train.shape)\n",
    "print('Train labels shape:', y_train.shape)\n",
    "print('Test data shape:', X_test.shape)\n",
    "print('Test labels shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "acc_score=accuracy_score(y_test,y_pred) \n",
    "print('accuracy_score: {0:.4f}'.format(acc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Labels = np.expand_dims(Labels, axis=1)\n",
    "Labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integer_encoded=Labels\n",
    "print(integer_encoded)\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)\n",
    "print(onehot_encoded.shape)\n",
    "\n",
    "\n",
    "validation_size = 0.20\n",
    "X_train, X_test, y_train, y_test = train_test_split(Features, onehot_encoded, test_size=validation_size, random_state=510) \n",
    "print(\"\\ntrain data shape =\",X_train.shape) \n",
    "print(\"train labels shape =\",y_train.shape) \n",
    "print(\"test data shape =\",X_test.shape) \n",
    "print(\"test labels shape =\",y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "acc_score=accuracy_score(y_test,y_pred) \n",
    "print('accuracy_score: {0:.4f}'.format(acc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = model_\n",
    "clf.fit(X_train,y_train)\n",
    "y_score = clf.predict_proba(X_test)\n",
    "print(np.array(y_score).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0=y_score[0][:,1]\n",
    "t1=y_score[1][:,1]\n",
    "t2=y_score[2][:,1]\n",
    "t3=y_score[3][:,1]\n",
    "t4=y_score[4][:,1]\n",
    "t5=y_score[5][:,1]\n",
    "\n",
    "p=[t0,t1,t2,t3,t4,t5]\n",
    "np.array(p).shape\n",
    "\n",
    "w=np.asmatrix(p).T\n",
    "w.shape\n",
    "\n",
    "y_score=np.squeeze(np.asarray(w))\n",
    "y_score.shape\n",
    "\n",
    "print(y_test)\n",
    "print(y_score)\n",
    "\n",
    "print(y_test.shape,y_score.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.style.palettes import LINE_COLOR\n",
    "\n",
    "name = \"Extra Trees model using 12 DB with 6 classes\"\n",
    "path_img_base = \"./images/Shoham\"\n",
    "if not os.path.exists(path_img_base):\n",
    "    os.makedirs(path_img_base)\n",
    "n_classes = y_test.shape[1]\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "#roc_auc, min_, max_ = ROC_CI_y_true_y_pred(tpr[\"micro\"],fpr[\"micro\"])\n",
    "#print(roc_auc, min_, max_) \n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "\n",
    "# Plot ROC curve\n",
    "#plt.figure(figsize=(10, 10))\n",
    "\n",
    "classes_names=[\"DB\",\"SS\",\"SW\",\"A\",\"I\",\"B\"]\n",
    "for i in range(n_classes):\n",
    "    score, ci_lower, ci_upper, scores = score_ci(y_test[:, i], y_score[:, i], \n",
    "                                                   score_fun=roc_auc_score,\n",
    "                                                   seed=42)\n",
    "\n",
    "    text=\"ROC curve with AUC of {:.3f} (95% CI: {:.3f}, {:.3f}). \"+\"Class \"+classes_names[i]\n",
    "\n",
    "    ax[0,1].plot(fpr[i], tpr[i], \n",
    "             label=text.format(score, ci_lower, ci_upper))\n",
    "\n",
    "ax[0,1].plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (AUC = {0:0.3f})'.format(roc_auc[\"micro\"]),\n",
    "         color='aqua', linestyle=':', linewidth=4)\n",
    "\n",
    "ax[0,1].plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (AUC = {0:0.3f})'.format(roc_auc[\"macro\"]),\n",
    "         color='darkorange', linestyle=':', linewidth=4)\n",
    "\n",
    "ax[0,1].tick_params(axis='x', which='major', labelsize=16) \n",
    "ax[0,1].tick_params(axis='y', which='major', labelsize=16)\n",
    "ax[0,1].plot([0, 1], [0, 1], linestyle=\":\", c=LINE_COLOR) \n",
    "ax[0,1].set_xlim([0.0, 1.0])\n",
    "ax[0,1].set_ylim([0.0, 1.0])\n",
    "ax[0,1].set_xlabel('False Positive Rate', fontsize=17)\n",
    "ax[0,1].set_ylabel('True Positive Rate', fontsize=17)\n",
    "ax[0,1].set_title(\"ROC Curves for {}\".format(name), fontsize=18)\n",
    "ax[0,1].legend(loc=\"lower right\", frameon=True, fontsize=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_ = ExtraTreesClassifier(n_estimators=100, max_depth=None, min_samples_split=3, random_state=17)\n",
    "model_ = ExtraTreesClassifier(n_estimators=200, max_depth=31, min_samples_split=3, random_state=125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "PATH = '../Databases/12DB_5FP.csv'\n",
    "\n",
    "Data = pd.read_csv(PATH, names=['Vsl', 'Vsg', 'VisL', 'VisG', 'DenL', 'DenG', 'ST', 'Ang', 'ID', 'Flow Pattern'], header=0)\n",
    "print('Data shape:', Data.shape)\n",
    "Data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the class distribution\n",
    "Data['Flow Pattern'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Train, test split\n",
    "features_list = ['Vsl', 'Vsg', 'VisL', 'VisG', 'DenL', 'DenG', 'ST', 'Ang', 'ID']\n",
    "Features = Data[features_list]\n",
    "Labels = Data['Flow Pattern']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Features, Labels, test_size=0.2, stratify=Labels, random_state=42)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "print('Train data shape:', X_train.shape)\n",
    "print('Train labels shape:', y_train.shape)\n",
    "print('Test data shape:', X_test.shape)\n",
    "print('Test labels shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "model = model_\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "acc_score=accuracy_score(y_test,y_pred) \n",
    "print('accuracy_score: {0:.4f}'.format(acc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Labels = np.expand_dims(Labels, axis=1)\n",
    "Labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integer_encoded=Labels\n",
    "print(integer_encoded)\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)\n",
    "print(onehot_encoded.shape)\n",
    "\n",
    "\n",
    "validation_size = 0.20\n",
    "X_train, X_test, y_train, y_test = train_test_split(Features, onehot_encoded, test_size=validation_size, random_state=510) \n",
    "print(\"\\ntrain data shape =\",X_train.shape) \n",
    "print(\"train labels shape =\",y_train.shape) \n",
    "print(\"test data shape =\",X_test.shape) \n",
    "print(\"test labels shape =\",y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "model = model_\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "acc_score=accuracy_score(y_test,y_pred) \n",
    "print('accuracy_score: {0:.4f}'.format(acc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = model_\n",
    "clf.fit(X_train,y_train)\n",
    "y_score = clf.predict_proba(X_test)\n",
    "print(np.array(y_score).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0=y_score[0][:,1]\n",
    "t1=y_score[1][:,1]\n",
    "t2=y_score[2][:,1]\n",
    "t3=y_score[3][:,1]\n",
    "t4=y_score[4][:,1]\n",
    "\n",
    "p=[t0,t1,t2,t3,t4]\n",
    "np.array(p).shape\n",
    "\n",
    "w=np.asmatrix(p).T\n",
    "w.shape\n",
    "\n",
    "y_score=np.squeeze(np.asarray(w))\n",
    "y_score.shape\n",
    "\n",
    "print(y_test)\n",
    "print(y_score)\n",
    "\n",
    "print(y_test.shape,y_score.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.style.palettes import LINE_COLOR\n",
    "\n",
    "name = \"Extra Trees model using 12 DB with 5 classes\"\n",
    "path_img_base = \"./images/Shoham\"\n",
    "if not os.path.exists(path_img_base):\n",
    "    os.makedirs(path_img_base)\n",
    "n_classes = y_test.shape[1]\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "#roc_auc, min_, max_ = ROC_CI_y_true_y_pred(tpr[\"micro\"],fpr[\"micro\"])\n",
    "#print(roc_auc, min_, max_) \n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "\n",
    "# Plot ROC curve\n",
    "#plt.figure(figsize=(10, 10))\n",
    "\n",
    "classes_names=[\"DB\",\"ST\",\"A\",\"I\",\"B\"]\n",
    "for i in range(n_classes):\n",
    "    score, ci_lower, ci_upper, scores = score_ci(y_test[:, i], y_score[:, i], \n",
    "                                                   score_fun=roc_auc_score,\n",
    "                                                   seed=42)\n",
    "\n",
    "    text=\"ROC curve with AUC of {:.3f} (95% CI: {:.3f}, {:.3f}). \"+\"Class \"+classes_names[i]\n",
    "\n",
    "    ax[1,0].plot(fpr[i], tpr[i], \n",
    "             label=text.format(score, ci_lower, ci_upper))\n",
    "\n",
    "ax[1,0].plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (AUC = {0:0.3f})'.format(roc_auc[\"micro\"]),\n",
    "         color='aqua', linestyle=':', linewidth=4)\n",
    "\n",
    "ax[1,0].plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (AUC = {0:0.3f})'.format(roc_auc[\"macro\"]),\n",
    "         color='darkorange', linestyle=':', linewidth=4)\n",
    "\n",
    "ax[1,0].tick_params(axis='x', which='major', labelsize=16) \n",
    "ax[1,0].tick_params(axis='y', which='major', labelsize=16)\n",
    "ax[1,0].plot([0, 1], [0, 1], linestyle=\":\", c=LINE_COLOR) \n",
    "ax[1,0].set_xlim([0.0, 1.0])\n",
    "ax[1,0].set_ylim([0.0, 1.0])\n",
    "ax[1,0].set_xlabel('False Positive Rate', fontsize=17)\n",
    "ax[1,0].set_ylabel('True Positive Rate', fontsize=17)\n",
    "ax[1,0].set_title(\"ROC Curves for {}\".format(name), fontsize=18)\n",
    "ax[1,0].legend(loc=\"lower right\", frameon=True, fontsize=15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = ExtraTreesClassifier(n_estimators=100, max_depth=None, min_samples_split=2, random_state=38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "PATH = '../Databases/12DB_3FP.csv'\n",
    "\n",
    "Data = pd.read_csv(PATH, names=['Vsl', 'Vsg', 'VisL', 'VisG', 'DenL', 'DenG', 'ST', 'Ang', 'ID', 'Flow Pattern'], header=0)\n",
    "print('Data shape:', Data.shape)\n",
    "Data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the class distribution\n",
    "Data['Flow Pattern'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Train, test split\n",
    "features_list = ['Vsl', 'Vsg', 'VisL', 'VisG', 'DenL', 'DenG', 'ST', 'Ang', 'ID']\n",
    "Features = Data[features_list]\n",
    "Labels = Data['Flow Pattern']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Features, Labels, test_size=0.2, stratify=Labels, random_state=42)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "print('Train data shape:', X_train.shape)\n",
    "print('Train labels shape:', y_train.shape)\n",
    "print('Test data shape:', X_test.shape)\n",
    "print('Test labels shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "model = model_\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "acc_score=accuracy_score(y_test,y_pred) \n",
    "print('accuracy_score: {0:.4f}'.format(acc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Labels = np.expand_dims(Labels, axis=1)\n",
    "Labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integer_encoded=Labels\n",
    "print(integer_encoded)\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)\n",
    "print(onehot_encoded.shape)\n",
    "\n",
    "\n",
    "validation_size = 0.20\n",
    "X_train, X_test, y_train, y_test = train_test_split(Features, onehot_encoded, test_size=validation_size, random_state=510) \n",
    "print(\"\\ntrain data shape =\",X_train.shape) \n",
    "print(\"train labels shape =\",y_train.shape) \n",
    "print(\"test data shape =\",X_test.shape) \n",
    "print(\"test labels shape =\",y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "model = model_\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "acc_score=accuracy_score(y_test,y_pred) \n",
    "print('accuracy_score: {0:.4f}'.format(acc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = model_\n",
    "clf.fit(X_train,y_train)\n",
    "y_score = clf.predict_proba(X_test)\n",
    "print(np.array(y_score).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0=y_score[0][:,1]\n",
    "t1=y_score[1][:,1]\n",
    "t2=y_score[2][:,1]\n",
    "\n",
    "p=[t0,t1,t2]\n",
    "np.array(p).shape\n",
    "\n",
    "w=np.asmatrix(p).T\n",
    "w.shape\n",
    "\n",
    "y_score=np.squeeze(np.asarray(w))\n",
    "y_score.shape\n",
    "\n",
    "print(y_test)\n",
    "print(y_score)\n",
    "\n",
    "print(y_test.shape,y_score.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.style.palettes import LINE_COLOR\n",
    "\n",
    "name = \"Extra Trees model using 12 DB with 3 classes\"\n",
    "path_img_base = \"./images/Shoham\"\n",
    "if not os.path.exists(path_img_base):\n",
    "    os.makedirs(path_img_base)\n",
    "n_classes = y_test.shape[1]\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "#roc_auc, min_, max_ = ROC_CI_y_true_y_pred(tpr[\"micro\"],fpr[\"micro\"])\n",
    "#print(roc_auc, min_, max_) \n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "\n",
    "# Plot ROC curve\n",
    "#plt.figure(figsize=(10, 10))\n",
    "\n",
    "classes_names=[\"Dispersed\",\"Segregate\",\"Intermittent\"]\n",
    "for i in range(n_classes):\n",
    "    score, ci_lower, ci_upper, scores = score_ci(y_test[:, i], y_score[:, i], \n",
    "                                                   score_fun=roc_auc_score,\n",
    "                                                   seed=42)\n",
    "\n",
    "    text=\"ROC curve with AUC of {:.3f} (95% CI: {:.3f}, {:.3f}). \"+\"Class \"+classes_names[i]\n",
    "\n",
    "    ax[1,1].plot(fpr[i], tpr[i], \n",
    "             label=text.format(score, ci_lower, ci_upper))\n",
    "\n",
    "ax[1,1].plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (AUC = {0:0.3f})'.format(roc_auc[\"micro\"]),\n",
    "         color='aqua', linestyle=':', linewidth=4)\n",
    "\n",
    "ax[1,1].plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (AUC = {0:0.3f})'.format(roc_auc[\"macro\"]),\n",
    "         color='darkorange', linestyle=':', linewidth=4)\n",
    "\n",
    "ax[1,1].tick_params(axis='x', which='major', labelsize=16) \n",
    "ax[1,1].tick_params(axis='y', which='major', labelsize=16)\n",
    "ax[1,1].plot([0, 1], [0, 1], linestyle=\":\", c=LINE_COLOR) \n",
    "ax[1,1].set_xlim([0.0, 1.0])\n",
    "ax[1,1].set_ylim([0.0, 1.0])\n",
    "ax[1,1].set_xlabel('False Positive Rate', fontsize=17)\n",
    "ax[1,1].set_ylabel('True Positive Rate', fontsize=17)\n",
    "ax[1,1].set_title(\"ROC Curves for {}\".format(name), fontsize=18)\n",
    "ax[1,1].legend(loc=\"lower right\", frameon=True, fontsize=13.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(path_img_base+'/ROC_CI_graphs'+'.pdf', format='pdf') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Example_RF_AUC_util_stats (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
