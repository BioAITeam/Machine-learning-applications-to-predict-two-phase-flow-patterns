{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K37OeDTJ3pxa"
   },
   "source": [
    "# NEURONAL NETWORKS FOR FLOW PATTERNS CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g72lntaxgL0s"
   },
   "outputs": [],
   "source": [
    "data_path = '../Databases/ShohamDB.csv' \n",
    "figures_path = './figures' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwWzJ3H7s1Vx"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M0gRVrcYsBs8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import time as tm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(figures_path):\n",
    "    os.makedirs(figures_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLPzaDX8s-7G"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uxvcA4gesIim"
   },
   "outputs": [],
   "source": [
    "## Loading Data\n",
    "\n",
    "# Velocity, Viscosity, Density, Surface Tension, Angle and Diameter\n",
    "dataset = pd.DataFrame(pd.read_csv(data_path), columns=['Vsl', 'Vsg', 'VisL', 'VisG', 'DenL', 'DenG', 'ST', 'Ang', 'ID', 'Flow Pattern']) \n",
    "\n",
    "# Summarize the Dataset \n",
    "print(\"shape of initial data =\",dataset.shape) \n",
    "# Class Distribution \n",
    "print(dataset.groupby('Flow Pattern').size()) \n",
    "# Leaving only the best training variables\n",
    "dataset = dataset.drop(['VisG', 'VisL','DenG', 'ST', 'DenL'], axis=1) #Delete this variables\n",
    "print(\"shape of selected data =\",dataset.shape) \n",
    "\n",
    "print(dataset.head()) \n",
    "\n",
    "# Split-out validation dataset \n",
    "array = dataset.values \n",
    "X = array[:,0:4] #Data or features \n",
    "Y = array[:,4]   #Label or classes \n",
    "validation_size = 0.20\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=validation_size, random_state=510) \n",
    "\n",
    "## Data preparation for neural networks\n",
    "X_train_ = X_train\n",
    "X_test_ = X_test\n",
    "\n",
    "#Renaming labels for y_train\n",
    "Yt = y_train.copy()\n",
    "for i in range(len(Yt)):\n",
    "    if Yt[i] == 'A':\n",
    "        Yt[i] = 0\n",
    "    elif Yt[i] == 'B':\n",
    "        Yt[i] = 1\n",
    "    elif Yt[i] == 'DB':\n",
    "        Yt[i] = 2\n",
    "    elif Yt[i] == 'I':\n",
    "        Yt[i] = 3\n",
    "    elif Yt[i] == 'SS':\n",
    "        Yt[i] = 4\n",
    "    elif Yt[i] == 'SW':\n",
    "        Yt[i] = 5\n",
    "    else:\n",
    "        print(\"Dimesion error\")\n",
    "\n",
    "Yt = np.array(np.eye(6)[np.array(Yt, dtype=int)], dtype=np.float32) #For the Softmax activation function\n",
    "\n",
    "#Renaming labels for y_train\n",
    "Yv = y_test.copy()\n",
    "for i in range(len(Yv)):\n",
    "    if Yv[i] == 'A':\n",
    "        Yv[i] = 0\n",
    "    elif Yv[i] == 'B':\n",
    "        Yv[i] = 1\n",
    "    elif Yv[i] == 'DB':\n",
    "        Yv[i] = 2\n",
    "    elif Yv[i] == 'I':\n",
    "        Yv[i] = 3\n",
    "    elif Yv[i] == 'SS':\n",
    "        Yv[i] = 4\n",
    "    elif Yv[i] == 'SW':\n",
    "        Yv[i] = 5\n",
    "    else:\n",
    "        print(\"Dimesion error\")\n",
    "\n",
    "Yv = np.array(np.eye(6)[np.array(Yv, dtype=int)], dtype=np.float32) #For the Softmax activation function\n",
    "\n",
    "X_train = np.expand_dims(X_train,axis=2)\n",
    "X_test  = np.expand_dims(X_test,axis=2)\n",
    "X_train = np.array(X_train, dtype=np.float64)\n",
    "X_test  = np.array(X_test, dtype=np.float64)\n",
    "y_train = Yt\n",
    "y_test = Yv\n",
    "print(\"\\ntrain data shape =\",X_train.shape) \n",
    "print(\"train labels shape =\",y_train.shape) \n",
    "print(\"test data shape =\",X_test.shape) \n",
    "print(\"test labels shape =\",y_test.shape) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGxKaezGUFIf"
   },
   "source": [
    "## CONVOLUTIONAL NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xCzBOggDUBku"
   },
   "outputs": [],
   "source": [
    "def modelCNN():\n",
    "    ip = tf.keras.layers.Input(shape=[X_train_.shape[1],1])\n",
    "    layers = tf.keras.layers.Conv1D(filters=32, kernel_size=(1), activation='relu')(ip) \n",
    "    layers = tf.keras.layers.Conv1D(filters=32, kernel_size=(1), activation='relu')(layers)\n",
    "    layers = tf.keras.layers.MaxPooling1D(1)(layers)\n",
    "    layers = tf.keras.layers.Conv1D(filters=32, kernel_size=(1), activation='relu')(layers) \n",
    "    layers = tf.keras.layers.Flatten()(layers)\n",
    "    layers = tf.keras.layers.Dense(512, activation=\"selu\")(layers) \n",
    "    layers = tf.keras.layers.Dense(256, activation=\"selu\")(layers)\n",
    "    layers = tf.keras.layers.Dense(128, activation=\"selu\")(layers)\n",
    "    layers = tf.keras.layers.Dense(64, activation=\"selu\")(layers) \n",
    "    x = tf.keras.layers.Dense(6, activation=\"softmax\", name=\"output_1\")(layers) \n",
    "    model = tf.keras.Model(inputs=ip, outputs=x) \n",
    "    optimizer = tf.keras.optimizers.RMSprop() \n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JosK_4BDUBrX"
   },
   "outputs": [],
   "source": [
    "modelCNN = modelCNN() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cxhhiKJbUCRM",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# BEST CNN MODEL\n",
    "epochs = 700\n",
    "batch_size=64\n",
    "\n",
    "lossTRAIN,accuracyTRAIN = modelCNN.evaluate(X_train, y_train, verbose=None)\n",
    "lossTEST,accuracyTEST   = modelCNN.evaluate(X_test, y_test, verbose=None)\n",
    "start_time = tm.time()\n",
    "history=modelCNN.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test),verbose=2)\n",
    "TIME = tm.time() - start_time\n",
    "print(\"\\nTime: {0:.4f} [seconds]\".format(TIME))\n",
    "\n",
    "with plt.style.context('seaborn-white'):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    #Plot training & validation accuracy values\n",
    "    plt.plot(np.concatenate([np.array([accuracyTRAIN]),np.array(history.history['accuracy'])],axis=0))\n",
    "    plt.plot(np.concatenate([np.array([accuracyTEST]), np.array(history.history['val_accuracy'])],axis=0))\n",
    "    plt.title('Accuracy Vs Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.grid('on')\n",
    "    plt.savefig(figures_path+\"/CNN_acc.pdf\", format='pdf')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    #Plot training & validation loss values\n",
    "    plt.plot(np.concatenate([np.array([lossTRAIN]),np.array(history.history['loss'])],axis=0))\n",
    "    plt.plot(np.concatenate([np.array([lossTEST]), np.array(history.history['val_loss'])],axis=0))\n",
    "    plt.title('Loss Vs Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.grid('on')\n",
    "    plt.savefig(figures_path+\"/CNN_loss.pdf\", format='pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxZoisO0sIvC"
   },
   "source": [
    "## ARTIFICIAL NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelANN():\n",
    "    ip = tf.keras.layers.Input(shape=[X_train_.shape[1],1])\n",
    "    layers = tf.keras.layers.Flatten()(ip)\n",
    "    layers = tf.keras.layers.Dense(512, activation=\"selu\")(layers) \n",
    "    layers = tf.keras.layers.Dense(256, activation=\"selu\")(layers)\n",
    "    layers = tf.keras.layers.Dense(128, activation=\"selu\")(layers)\n",
    "    layers = tf.keras.layers.Dense(64, activation=\"selu\")(layers) \n",
    "    x = tf.keras.layers.Dense(6, activation=\"softmax\", name=\"output_1\")(layers)  \n",
    "    model = tf.keras.Model(inputs=ip, outputs=x) \n",
    "    optimizer = tf.keras.optimizers.RMSprop() \n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QAeOLb003DPB"
   },
   "outputs": [],
   "source": [
    "modelANN = modelANN() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UlWhgyVFsMEn",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# BEST ANN MODEL\n",
    "epochs = 1200\n",
    "batch_size=64\n",
    "\n",
    "lossTRAIN,accuracyTRAIN = modelANN.evaluate(X_train, y_train, verbose=None)\n",
    "lossTEST,accuracyTEST   = modelANN.evaluate(X_test, y_test, verbose=None)\n",
    "start_time = tm.time()\n",
    "history=modelANN.fit(X_train, y_train, epochs=epochs, batch_size=64, validation_data=(X_test, y_test),verbose=2)\n",
    "TIME = tm.time() - start_time\n",
    "print(\"\\nTime: {0:.4f} [seconds]\".format(TIME))\n",
    "\n",
    "with plt.style.context('seaborn-white'):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    #Plot training & validation accuracy values\n",
    "    plt.plot(np.concatenate([np.array([accuracyTRAIN]),np.array(history.history['accuracy'])],axis=0))\n",
    "    plt.plot(np.concatenate([np.array([accuracyTEST]), np.array(history.history['val_accuracy'])],axis=0))\n",
    "    plt.title('Accuracy Vs Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.grid('on')\n",
    "    plt.savefig(figures_path+\"/ANN_acc.pdf\", format='pdf')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    #Plot training & validation loss values\n",
    "    plt.plot(np.concatenate([np.array([lossTRAIN]),np.array(history.history['loss'])],axis=0))\n",
    "    plt.plot(np.concatenate([np.array([lossTEST]), np.array(history.history['val_loss'])],axis=0))\n",
    "    plt.title('Loss Vs Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.grid('on')\n",
    "    plt.savefig(figures_path+\"/ANN_loss.pdf\", format='pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tqT-AlrDXbNn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Deep_Learning_Models.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
